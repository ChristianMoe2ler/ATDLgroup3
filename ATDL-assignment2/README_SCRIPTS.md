# ATDL Assignment 2 Documentation

## Overview

This directory contains shell scripts and Python utilities for running DDPM segmentation experiments with various configurations.
## Requirements

python 3.8.12
pytorch 1.11.0-1

All other dependencies will be install during the script execution. For more information look at the function "setup_environment" in common_functions.sh.

## Dataset Types Explained

There are **three types of datasets** used in these scripts:

1. **Real Data**: `datasets/<name>/real/`
   - Manually annotated real images
   - Downloaded via: `datasets/download_datasets.sh`
   - Used by most CV scripts (e.g., `cv_5fold_custom_training_size.sh`)

2. **DDPM Synthetic Data**: `datasets/<name>/ddpm/`
   - Generated by DDPM diffusion models
   - Included in standard dataset download
   - Used by: `cv_5fold_ddpm_synthetic.sh`, `synth_to_real_generalization.sh`
   - **✅ Fully supported by optimized scripts**

3. **GAN Synthetic Data**: `synthetic_datasets/gan/<name>/`
   - Generated by DatasetGAN
   - Requires separate download via `synthetic_datasets/gan/download_synthetic_dataset.sh`
   - Used by: `cv_5fold_gan_pretarred.sh`, `cv_5fold_gan_embedded_splitter.sh`
   - **⚠️ Not supported by optimized scripts** (kept for reference only)

## Script Descriptions

### **common_functions.sh**
Shared library containing reusable functions:
- `setup_environment()` - Install Python dependencies
- `clone_repository()` - Clone ddpm-segmentation repo
- `download_checkpoint()` - Download DDPM checkpoints
- `download_datasets()` - Download datasets
- `verify_directories()` - Validate train/test directories exist
- `count_images()` - Count images in directory
- `extract_config_params()` - Parse JSON config parameters
- `create_temp_config()` - Generate temporary config files
- `run_training()` - Execute training with consistent flags
- `cleanup_models()` - Remove .pth files to save space
- `print_results_summary()` - Display results location

### **split_dataset_lib.py**
Shared Python library for dataset splitting:
- `get_image_files()` - Find all images in directory
- `copy_image_with_label()` - Copy image + .npy annotation
- `copy_images_batch()` - Copy multiple images efficiently
- `collect_images_from_dirs()` - Gather images from train/test
- `create_fold_directories()` - Create fold_N/train and fold_N/test
- `print_fold_info()` - Display fold statistics
- `split_maintain_ratio()` - Split maintaining original train/test ratio
- `split_custom_train_size()` - Split with custom sizes

**Usage:**
```bash
# Maintain original train/test ratio (real data)
python3 split_dataset_optimized.py \
    --dataset bedroom_28 \
    --n_folds 5 \
    --seed 42 \
    --dataset_type real

# Custom training size (real data)
python3 split_dataset_optimized.py \
    --dataset bedroom_28 \
    --n_folds 5 \
    --train_size 10 \
    --seed 42 \
    --dataset_type real

# DDPM synthetic data
python3 split_dataset_optimized.py \
    --dataset bedroom_28 \
    --n_folds 5 \
    --seed 42 \
    --dataset_type ddpm
```

### **cv_5fold_optimized.sh**
Template script demonstrating how to use `common_functions.sh` library.

**Usage:**
```bash
# Default configuration
bash cv_5fold_optimized.sh

# Custom configuration via environment variables
DATASET=cat_15 CHECKPOINT=lsun_cat N_FOLDS=5 TRAIN_SIZE=10 bash cv_5fold_optimized.sh

# Dry run
bash cv_5fold_optimized.sh --dry-run
```

---

## Python Utilities

### **modify_classifier.py**
Modifies `src/pixel_classifier.py` to use different MLP architectures. It requires first to have downloaded the ddpm-segmentation github repo and going into the top level directory

**Architectures:**
- **baseline:** Original architecture
- **wider:** Increases hidden layer dimensions
- **deeper:** Adds more layers

**Usage:**
```bash
python3 modify_classifier.py deeper
```

---

## Common Configuration Parameters

Most scripts share these parameters:

| Parameter | Default | Description |
|-----------|---------|-------------|
| DATASET | bedroom_28 | Dataset name (bedroom_28, cat_15, horse_21, ffhq_34) |
| CHECKPOINT | lsun_bedroom | DDPM checkpoint (lsun_bedroom, lsun_cat, lsun_horse, ffhq) |
| N_FOLDS | 5 | Number of folds for cross-validation |
| SEED | 42 | Random seed for reproducibility |
| MODEL_NUM | 10 | Number of MLP heads in ensemble |
| TRAINING_IMAGES | - | Number of training images (if specified) |
| DRY_RUN | false | If true, print commands without executing |

---

## Workflow Patterns

### Pattern 1: Standard K-Fold CV
```bash
# Setup → Download → Split → Train on each fold → Summarize
bash cv_5fold_optimized.sh
```

### Pattern 2: Sensitivity Analysis
```bash
# Setup → Download → Patch code → Test all block/timestep combinations → CSV output
bash sensitivity_blocks_timesteps.sh
```

### Pattern 3: Synthetic Data Experiments
```bash
# Setup → Download GAN data → Split synthetic → Train → Test
bash cv_5fold_gan_embedded_splitter.sh
```

### Pattern 4: Transfer Learning
```bash
# Setup → Download both synthetic and real → Train on synthetic → Test on real
bash synth_to_real_generalization.sh
```

---

## Best Practices

1. **Use optimized scripts for new experiments:**
   - Copy `cv_5fold_optimized.sh` as a template
   - Modify configuration section
   - Script automatically uses shared functions

2. **Use split_dataset_optimized.py for data splitting:**
   - Single script replaces all three original splitters
   - Clearer interface with --train_size and --dataset_type flags

3. **Run dry-run first:**
   ```bash
   bash your_script.sh --dry-run
   ```

