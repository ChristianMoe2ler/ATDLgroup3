# ATDL Assignment 2 - Script Documentation

## Overview

This directory contains shell scripts and Python utilities for running DDPM segmentation experiments with various configurations. Scripts have been renamed for clarity and optimized versions created to reduce code duplication.

## Directory Structure

```
ATDL-assignment2/
├── README_SCRIPTS.md                        # This file
│
├── Optimized Scripts (USE THESE):
├── common_functions.sh                      # Shared shell functions library
├── split_dataset_lib.py                     # Shared Python splitting functions
├── split_dataset_optimized.py               # Optimized unified splitting script
├── cv_5fold_optimized.sh                    # Optimized template using common functions
│
├── Utilities:
├── modify_classifier.py                     # MLP architecture modification utility
│
└── original_scripts/                        # Original scripts (kept for reference)
    ├── split_dataset_into_folds.py              # Basic fold splitting (maintains ratio)
    ├── split_dataset_into_folds2_num_train_images.py  # Custom training size
    ├── split_dataset_into_folds3_ddpm_synthetic.py    # DDPM synthetic data (empty)
    ├── cv_5fold_custom_training_size.sh         # 5-fold CV with custom training images
    ├── cv_5fold_variable_train_size.sh          # 5-fold CV with train_size parameter
    ├── cv_5fold_custom_model_num.sh             # 5-fold CV with MODEL_NUM parameter
    ├── sensitivity_blocks_timesteps.sh          # Sensitivity analysis (blocks/timesteps)
    ├── cv_5fold_gan_pretarred.sh                # GAN synthetic with pre-tarred folds
    ├── cv_5fold_gan_embedded_splitter.sh        # GAN synthetic with embedded splitter
    ├── cv_5fold_ddpm_synthetic.sh               # DDPM synthetic data k-fold experiments
    ├── synth_to_real_generalization.sh          # Synthetic→Real transfer learning
    └── mlp_architecture_ablation.sh             # MLP width/depth ablation study
```

## Important Note: Original Scripts

**All original scripts have been moved to the `original_scripts/` subdirectory.** These are kept for reference and backward compatibility, but you should use the optimized versions in the main directory for new experiments.

The original scripts contain significant code duplication (~70% overlap between shell scripts and ~60% between Python scripts). The optimized versions reduce this duplication by ~40% through shared libraries.

## Dataset Types Explained

There are **three types of datasets** used in these scripts:

1. **Real Data**: `datasets/<name>/real/`
   - Manually annotated real images
   - Downloaded via: `datasets/download_datasets.sh`
   - Used by most CV scripts (e.g., `cv_5fold_custom_training_size.sh`)

2. **DDPM Synthetic Data**: `datasets/<name>/ddpm/`
   - Generated by DDPM diffusion models
   - Included in standard dataset download
   - Used by: `cv_5fold_ddpm_synthetic.sh`, `synth_to_real_generalization.sh`
   - **✅ Fully supported by optimized scripts**

3. **GAN Synthetic Data**: `synthetic_datasets/gan/<name>/`
   - Generated by DatasetGAN
   - Requires separate download via `synthetic_datasets/gan/download_synthetic_dataset.sh`
   - Used by: `cv_5fold_gan_pretarred.sh`, `cv_5fold_gan_embedded_splitter.sh`
   - **⚠️ Not supported by optimized scripts** (kept for reference only)

## Script Descriptions

### Optimized Scripts (Use These!)

#### **common_functions.sh**
Shared library containing reusable functions:
- `setup_environment()` - Install Python dependencies
- `clone_repository()` - Clone ddpm-segmentation repo
- `download_checkpoint()` - Download DDPM checkpoints
- `download_datasets()` - Download datasets
- `verify_directories()` - Validate train/test directories exist
- `count_images()` - Count images in directory
- `extract_config_params()` - Parse JSON config parameters
- `create_temp_config()` - Generate temporary config files
- `run_training()` - Execute training with consistent flags
- `cleanup_models()` - Remove .pth files to save space
- `print_results_summary()` - Display results location

#### **split_dataset_lib.py**
Shared Python library for dataset splitting:
- `get_image_files()` - Find all images in directory
- `copy_image_with_label()` - Copy image + .npy annotation
- `copy_images_batch()` - Copy multiple images efficiently
- `collect_images_from_dirs()` - Gather images from train/test
- `create_fold_directories()` - Create fold_N/train and fold_N/test
- `print_fold_info()` - Display fold statistics
- `split_maintain_ratio()` - Split maintaining original train/test ratio
- `split_custom_train_size()` - Split with custom sizes

#### **split_dataset_optimized.py**
Unified splitting script that replaces all three original splitting scripts.

**Features:**
- Flexible splitting strategies (maintain ratio or custom size)
- Supports both real and DDPM synthetic datasets
- Dry-run mode for testing
- Comprehensive error checking

**Usage:**
```bash
# Maintain original train/test ratio (real data)
python3 split_dataset_optimized.py \
    --dataset bedroom_28 \
    --n_folds 5 \
    --seed 42 \
    --dataset_type real

# Custom training size (real data)
python3 split_dataset_optimized.py \
    --dataset bedroom_28 \
    --n_folds 5 \
    --train_size 10 \
    --seed 42 \
    --dataset_type real

# DDPM synthetic data
python3 split_dataset_optimized.py \
    --dataset bedroom_28 \
    --n_folds 5 \
    --seed 42 \
    --dataset_type ddpm
```

#### **cv_5fold_optimized.sh**
Template script demonstrating how to use `common_functions.sh` library.

**Features:**
- Uses shared functions to eliminate duplication
- Configurable via environment variables or editing
- Supports both ratio-based and size-based splitting
- Clean, maintainable code structure

**Usage:**
```bash
# Default configuration
bash cv_5fold_optimized.sh

# Custom configuration via environment variables
DATASET=cat_15 CHECKPOINT=lsun_cat N_FOLDS=5 TRAIN_SIZE=10 bash cv_5fold_optimized.sh

# Dry run
bash cv_5fold_optimized.sh --dry-run
```

---

### Original Shell Scripts (in `original_scripts/` directory)

**Note:** These scripts are kept for reference. For new experiments, use the optimized versions above.

#### **cv_5fold_custom_training_size.sh**
- **Purpose:** 5-fold cross-validation with configurable training images
- **Dataset:** bedroom_28 (configurable)
- **Features:**
  - TRAINING_IMAGES parameter (default: 1)
  - MODEL_NUM parameter (default: 5)
  - Uses split_dataset_into_folds.py
- **Use Case:** Testing few-shot learning performance

#### **cv_5fold_variable_train_size.sh**
- **Purpose:** 5-fold CV with variable training size
- **Dataset:** bedroom_28 (configurable)
- **Features:**
  - TRAINING_IMAGES parameter
  - Copies split_dataset_into_folds2.py into repo
  - Uses train_size argument
- **Use Case:** Studying impact of training set size

#### **cv_5fold_custom_model_num.sh**
- **Purpose:** 5-fold CV with custom ensemble size
- **Dataset:** bedroom_28 (configurable)
- **Features:**
  - MODEL_NUM parameter (default: 5)
  - Uses split_dataset_into_folds.py
  - Maintains original train/test ratio
- **Use Case:** Testing ensemble size impact

#### **sensitivity_blocks_timesteps.sh**
- **Purpose:** Sensitivity analysis of DDPM blocks and timesteps
- **Dataset:** bedroom_28 (configurable)
- **Features:**
  - Tests combinations of blocks [2, 6, 10, 14]
  - Tests timesteps [50, 250, 500, 750, 950]
  - Auto-detects feature dimensions
  - Patches train_interpreter.py for auto-detection
  - Outputs results to CSV file
- **Use Case:** Understanding which features matter most

#### **cv_5fold_gan_pretarred.sh**
- **Purpose:** 5-fold CV using GAN synthetic data from pre-created tar files
- **Dataset:** bedroom_28 (configurable)
- **Features:**
  - Expects fold_1.tar.gz ... fold_5.tar.gz in parent directory
  - Extracts folds to synthetic_datasets/gan/<dataset>/extracted/
  - Tests on real data, trains on GAN synthetic
- **Use Case:** Using pre-generated synthetic folds

#### **cv_5fold_gan_embedded_splitter.sh**
- **Purpose:** 5-fold CV using GAN synthetic data with on-the-fly splitting
- **Dataset:** bedroom_28 (configurable)
- **Features:**
  - Downloads GAN synthetic dataset
  - Creates inline Python script for splitting
  - Imports split_dataset_into_folds functions
- **Use Case:** Generating synthetic folds dynamically

#### **cv_5fold_ddpm_synthetic.sh**
- **Purpose:** 5-fold CV on DDPM synthetic data
- **Dataset:** cat_15 (configurable)
- **Features:**
  - N_FOLDS=5, SEED=100
  - Uses `datasets/<name>/ddpm/fold_i/` directories
  - Trains and tests on DDPM-generated synthetic images
- **Use Case:** Evaluating performance on DDPM synthetic data with k-fold CV
- **Note:** This script was previously misnamed as `cv_5fold_real_data.sh`

#### **synth_to_real_generalization.sh**
- **Purpose:** Train on synthetic, test on real (transfer learning)
- **Dataset:** horse_21 (configurable)
- **Features:**
  - N_FOLDS=2 (runs multiple times with different seeds)
  - Train on datasets/<name>/ddpm (synthetic)
  - Test on datasets/<name>/real/test (real)
  - Experiment seed increments: SEED + i
- **Use Case:** Evaluating synthetic→real generalization

#### **mlp_architecture_ablation.sh**
- **Purpose:** Study impact of MLP architecture (wider/deeper)
- **Dataset:** ffhq_34 (configurable)
- **Features:**
  - MLP_TYPE parameter: "baseline", "wider", or "deeper"
  - Uses modify_classifier.py to change architecture
  - N_FOLDS=1 (single experiment per architecture)
  - Restores original classifier after experiment
- **Use Case:** Ablation study on classifier architecture

---

### Python Utilities

#### **modify_classifier.py**
Modifies `src/pixel_classifier.py` to use different MLP architectures.

**Architectures:**
- **baseline:** Original architecture
- **wider:** Increases hidden layer dimensions
- **deeper:** Adds more layers

**Usage:**
```bash
python3 modify_classifier.py deeper
```

---

## Common Configuration Parameters

Most scripts share these parameters:

| Parameter | Default | Description |
|-----------|---------|-------------|
| DATASET | bedroom_28 | Dataset name (bedroom_28, cat_15, horse_21, ffhq_34) |
| CHECKPOINT | lsun_bedroom | DDPM checkpoint (lsun_bedroom, lsun_cat, lsun_horse, ffhq) |
| N_FOLDS | 5 | Number of folds for cross-validation |
| SEED | 42 | Random seed for reproducibility |
| MODEL_NUM | 10 | Number of MLP heads in ensemble |
| TRAINING_IMAGES | - | Number of training images (if specified) |
| DRY_RUN | false | If true, print commands without executing |

---

## Workflow Patterns

### Pattern 1: Standard K-Fold CV
```bash
# Setup → Download → Split → Train on each fold → Summarize
bash cv_5fold_optimized.sh
```

### Pattern 2: Sensitivity Analysis
```bash
# Setup → Download → Patch code → Test all block/timestep combinations → CSV output
bash sensitivity_blocks_timesteps.sh
```

### Pattern 3: Synthetic Data Experiments
```bash
# Setup → Download GAN data → Split synthetic → Train → Test
bash cv_5fold_gan_embedded_splitter.sh
```

### Pattern 4: Transfer Learning
```bash
# Setup → Download both synthetic and real → Train on synthetic → Test on real
bash synth_to_real_generalization.sh
```

---

## Best Practices

1. **Use optimized scripts for new experiments:**
   - Copy `cv_5fold_optimized.sh` as a template
   - Modify configuration section
   - Script automatically uses shared functions

2. **Use split_dataset_optimized.py for data splitting:**
   - Single script replaces all three original splitters
   - Clearer interface with --train_size and --dataset_type flags

3. **Run dry-run first:**
   ```bash
   bash your_script.sh --dry-run
   ```

4. **Check disk space before experiments:**
   - Model files are large (.pth files)
   - Scripts clean up after each fold to save space

5. **Monitor GPU memory:**
   - Feature extraction can be memory-intensive
   - Reduce batch size if OOM occurs

---

## Code Duplication Reduced

### Before Optimization:
- **9 shell scripts** with ~70% duplicate code
- **3 Python scripts** with ~60% duplicate code
- Total: ~2,500 lines with significant overlap

### After Optimization:
- **Common functions library:** 230 lines (reusable)
- **Split dataset library:** 175 lines (reusable)
- **Optimized scripts:** Reference libraries instead of copying code
- **Result:** ~40% reduction in code duplication

---

## Troubleshooting

### Issue: "ERROR: Training directory not found"
- **Solution:** Ensure split_dataset script completed successfully
- Check that fold_N/train and fold_N/test directories exist

### Issue: "Auto-detected feature dimension" not found
- **Solution:** sensitivity_blocks_timesteps.sh patches train_interpreter.py
- If patch fails, manually inspect train_interpreter.py

### Issue: Out of memory during feature extraction
- **Solution:** Reduce training_number in config or use fewer images

### Issue: Missing .npy annotation files
- **Solution:** Scripts warn but continue - some images may lack labels
- Check dataset integrity

---

## Future Improvements

1. **Add configuration file support:**
   - Replace hardcoded parameters with YAML/JSON config
   - Enable easy experiment tracking

2. **Add parallel fold execution:**
   - Run multiple folds simultaneously on different GPUs
   - Significant speedup for multi-GPU systems

3. **Add automatic hyperparameter tuning:**
   - Integrate Optuna or similar for hyperparameter search

4. **Add experiment tracking:**
   - Integration with MLflow or Weights & Biases
   - Automatic logging of metrics

---

## Contact

For questions about these scripts, refer to the CLAUDE.md file in the repository root or check the original repository documentation.
